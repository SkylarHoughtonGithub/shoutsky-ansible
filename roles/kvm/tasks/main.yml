#SPDX-License-Identifier: MIT-0
---
# tasks file for kvm

# Step 1: Check for virtualization extensions
- name: Check for Intel VT-x or AMD-V extensions
  shell: grep -E 'vmx|svm' /proc/cpuinfo
  register: virt_extensions
  failed_when: virt_extensions.rc != 0
  changed_when: false

- name: Display virtualization support status
  debug:
    msg: "Virtualization extensions are available"
  when: virt_extensions.rc == 0

# Step 2: Install KVM and verify package health
- name: Install KVM and related packages
  package:
    name:
      - qemu-kvm
      - libvirt
      - libvirt-client
      - libvirt-daemon
      - libvirt-daemon-driver-qemu
      - libvirt-daemon-driver-network
      - libvirt-daemon-driver-storage
      - virt-install
      - virt-manager
      - virt-top
      - bridge-utils
      - libguestfs-tools
    state: present

- name: Check if KVM modules are loaded
  shell: lsmod | grep -i kvm
  register: kvm_modules
  failed_when: kvm_modules.rc != 0
  changed_when: false

- name: Display KVM module status
  debug:
    msg: "KVM modules are loaded properly"
  when: kvm_modules.rc == 0

# Step 3: Start and enable libvirt daemon
- name: Enable and start libvirtd service
  systemd:
    name: libvirtd
    state: started
    enabled: yes

- name: Verify libvirtd service status
  systemd:
    name: libvirtd
  register: libvirtd_status
  failed_when: not libvirtd_status.status.ActiveState == "active"
  changed_when: false

# Step 4: Install Virtual Machine Manager GUI tools and Cockpit
- name: Install virt-manager GUI and Cockpit
  package:
    name:
      - virt-manager
      - virt-viewer
      - cockpit
      - cockpit-machines
      - cockpit-podman
      - cockpit-system
    state: present

- name: Enable and start Cockpit service
  systemd:
    name: cockpit.socket
    state: started
    enabled: yes

# Step 5: Create and Configure Bridge Networking
- name: Install network-manager packages
  package:
    name:
      - NetworkManager
      - NetworkManager-libnm
    state: present

- name: Create bridge interface file
  template:
    src: templates/bridge-interface.xml.j2
    dest: /tmp/bridge.xml
    mode: '0644'

- name: Define bridge network in libvirt
  shell: virsh net-define /tmp/bridge.xml
  register: bridge_define
  failed_when: bridge_define.rc != 0 and "already exists" not in bridge_define.stderr
  changed_when: bridge_define.rc == 0

- name: Start bridge network
  shell: virsh net-start openshift-bridge
  register: bridge_start
  failed_when: bridge_start.rc != 0 and "already active" not in bridge_start.stderr
  changed_when: bridge_start.rc == 0

- name: Set bridge network to autostart
  shell: virsh net-autostart openshift-bridge
  register: bridge_autostart
  failed_when: bridge_autostart.rc != 0
  changed_when: bridge_autostart.rc == 0

# Step 6: Verify bridge network health
- name: Verify bridge network is active
  shell: virsh net-list --all | grep openshift-bridge | grep 'active'
  register: bridge_status
  failed_when: bridge_status.rc != 0
  changed_when: false

- name: Get bridge network details
  shell: virsh net-info openshift-bridge
  register: bridge_info
  failed_when: bridge_info.rc != 0
  changed_when: false

- name: Display bridge network information
  debug:
    var: bridge_info.stdout_lines

# Step 7: Setup bastion VM
- name: Create bastion VM disk
  shell: >
    qemu-img create -f qcow2 {{ libvirt_pool_dir }}/bastion.qcow2 {{ bastion_disk_size }}
  args:
    creates: "{{ libvirt_pool_dir }}/bastion.qcow2"

- name: Copy cloud-init config for bastion
  template:
    src: templates/bastion-cloud-init.yml.j2
    dest: /tmp/bastion-cloud-init.yml
    mode: '0644'

- name: Create bastion VM
  shell: >
    virt-install
    --name bastion
    --memory {{ bastion_memory }}
    --vcpus {{ bastion_vcpus }}
    --disk {{ libvirt_pool_dir }}/bastion.qcow2,format=qcow2
    --os-variant rhel8.5
    --network network=openshift-bridge,model=virtio
    --graphics none
    --noautoconsole
    --location {{ rhel_iso_location }}
    --extra-args "console=ttyS0"
    --initrd-inject /tmp/bastion-cloud-init.yml
  args:
    creates: "/etc/libvirt/qemu/bastion.xml"
  register: bastion_install
  failed_when: bastion_install.rc != 0 and "domain 'bastion' already exists" not in bastion_install.stderr
  changed_when: bastion_install.rc == 0 and "domain 'bastion' already exists" not in bastion_install.stderr

# Step 8: Verify bastion VM health
- name: Wait for bastion VM to be running
  shell: virsh domstate bastion
  register: bastion_state
  until: bastion_state.stdout == "running"
  retries: 30
  delay: 10
  changed_when: false

- name: Get bastion VM IP address
  shell: >
    virsh domifaddr bastion | grep -oE "\b([0-9]{1,3}\.){3}[0-9]{1,3}\b"
  register: bastion_ip
  until: bastion_ip.stdout != ""
  retries: 30
  delay: 10
  changed_when: false

- name: Wait for SSH on bastion VM
  wait_for:
    host: "{{ bastion_ip.stdout }}"
    port: 22
    delay: 10
    timeout: 300

- name: Add bastion host to inventory
  add_host:
    name: bastion
    groups: bastion_servers
    ansible_host: "{{ bastion_ip.stdout }}"
    ansible_user: "{{ bastion_user }}"
    ansible_ssh_private_key_file: "{{ bastion_ssh_key }}"

- name: Install OpenShift client tools on bastion
  delegate_to: bastion
  package:
    name:
      - openshift-clients
      - python3-openshift
    state: present

# Step 9: Setup master OpenShift nodes
- name: Create master node disks
  shell: >
    qemu-img create -f qcow2 {{ libvirt_pool_dir }}/master{{ item }}.qcow2 {{ master_disk_size }}
  args:
    creates: "{{ libvirt_pool_dir }}/master{{ item }}.qcow2"
  with_sequence: start=1 end={{ master_count }}

- name: Create master node VMs
  shell: >
    virt-install
    --name master{{ item }}
    --memory {{ master_memory }}
    --vcpus {{ master_vcpus }}
    --disk {{ libvirt_pool_dir }}/master{{ item }}.qcow2,format=qcow2
    --os-variant rhel8.5
    --network network=openshift-bridge,model=virtio
    --graphics none
    --noautoconsole
    --pxe
    --boot network,hd,menu=on
  args:
    creates: "/etc/libvirt/qemu/master{{ item }}.xml"
  with_sequence: start=1 end={{ master_count }}
  register: master_install
  failed_when: master_install.rc != 0 and "domain 'master" + item + "' already exists" not in master_install.stderr
  changed_when: master_install.rc == 0 and "domain 'master" + item + "' already exists" not in master_install.stderr

# Step 10: Verify master node health
- name: Wait for master nodes to be running
  shell: virsh domstate master{{ item }}
  register: master_state
  until: master_state.stdout == "running"
  retries: 30
  delay: 10
  with_sequence: start=1 end={{ master_count }}
  changed_when: false

# Step 11: Setup worker OpenShift nodes
- name: Create worker node disks
  shell: >
    qemu-img create -f qcow2 {{ libvirt_pool_dir }}/worker{{ item }}.qcow2 {{ worker_disk_size }}
  args:
    creates: "{{ libvirt_pool_dir }}/worker{{ item }}.qcow2"
  with_sequence: start=1 end={{ worker_count }}

- name: Create worker node VMs
  shell: >
    virt-install
    --name worker{{ item }}
    --memory {{ worker_memory }}
    --vcpus {{ worker_vcpus }}
    --disk {{ libvirt_pool_dir }}/worker{{ item }}.qcow2,format=qcow2
    --os-variant rhel8.5
    --network network=openshift-bridge,model=virtio
    --graphics none
    --noautoconsole
    --pxe
    --boot network,hd,menu=on
  args:
    creates: "/etc/libvirt/qemu/worker{{ item }}.xml"
  with_sequence: start=1 end={{ worker_count }}
  register: worker_install
  failed_when: worker_install.rc != 0 and "domain 'worker" + item + "' already exists" not in worker_install.stderr
  changed_when: worker_install.rc == 0 and "domain 'worker" + item + "' already exists" not in worker_install.stderr

# Step 12: Verify worker node health
- name: Wait for worker nodes to be running
  shell: virsh domstate worker{{ item }}
  register: worker_state
  until: worker_state.stdout == "running"
  retries: 30
  delay: 10
  with_sequence: start=1 end={{ worker_count }}
  changed_when: false